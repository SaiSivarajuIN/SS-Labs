<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SS Labs | GGUF</title>
    <link rel="stylesheet" href="../static/styles.css">
    <link rel="icon" href="../static/logo/logo.png" type="image/png">
</head>
<body>
    <header class="page-header">
        <div class="menu-toggle" id="menu-toggle">
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M4 6H20M4 12H20M4 18H20" stroke="#333" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
            </svg>
        </div>
        <img src="../static/logo/logo.png" alt="SS Labs Logo" class="logo">
        <h1>SS Labs ‚Äî GGUF<span class="beta-tag">Lab Beta</span></h1>
    </header>

    <!-- Roadmap Navigation -->
    <nav class="roadmap" id="roadmap">
        <ul>
            <li><a href="#overview">Overview</a></li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Mistral AI</a>
                <ul class="dropdown-menu">
                    <li><a href="#mistral">Mistral-7b-v0.3-bnb-4bit-GGUF</a></li>
                    <li><a href="#mistral-nemo">Mistral-Nemo-Instruct-2407-GGUF</a></li>
                </ul>
            </li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Llama</a>
                <ul class="dropdown-menu">
                    <li><a href="#llama3">Llama-3-8b-bnb-4bit-GGUF</a></li>
                    <li><a href="#llama3.2">Llama-3.2-1B-Instruct-GGUF</a></li>
                </ul>
            </li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Microsoft Phi</a>
                <ul class="dropdown-menu">
                    <li><a href="#phi">Phi-3.5-mini-instruct-GGUF</a></li>
                    <li><a href="#phi-4k">Phi-3-mini-4k-instruct-GGUF</a></li>
                </ul>
            </li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Falcon</a>
                <ul class="dropdown-menu">
                    <li><a href="#falcon">Falcon-H1-0.5B-Instruct-GGUF</a></li>
                </ul>
            </li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Qwen</a>
                <ul class="dropdown-menu">
                    <li><a href="#qwen">Qwen3-4B-Instruct-2507-GGUF</a></li>
                </ul>
            </li>
            <li class="dropdown-container">
                <a class="dropdown-toggle">Liquid AI</a>
                <ul class="dropdown-menu">
                    <li><a href="#lfm">LFM2-2.6B-GGUF</a></li>
                </ul>
            </li>
            <li><a href="#performance">Model Comparison</a></li>
            <li><a href="#finetuning">Fine-tuning</a></li>
            <li><a href="#about">About SS Labs</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>
    
    <div class="main-content">
        <div class="content-wrapper">
            <!--- Main Content -->
            <h2>Welcome note</h2>
            <div class="spacer">
                <p>Welcome to <b>SS Labs</b>, a research and deployment group focused on efficient LLM conversions, 
                    fine-tuning, and GGUF model optimization for edge and cloud inference.
                </p>
                <blockquote><a href="https://huggingface.co/ss-lab" target="_blank">View on Hugging Face</a></blockquote>
            </div>

            <!-- Model Overview -->
            <h2 id="overview">Overview</h2>
            <ul class="model-overview-list">
                <li><b><a href="#mistral">Mistral-7b-v0.3-bnb-4bit-GGUF</a></b></li>
                <li><b><a href="#mistral-nemo">Mistral-Nemo-Instruct-2407-GGUF</a></b></li>
                <li><b><a href="#phi">Phi-3.5-mini-instruct-GGUF</a></b></li>
                <li><b><a href="#phi-4k">Phi-3-mini-4k-instruct-GGUF</a></b></li>
                <li><b><a href="#llama3">Llama-3-8b-bnb-4bit-GGUF</a></b></li>
                <li><b><a href="#llama3.2">Llama-3.2-1B-Instruct-GGUF</a></b></li>
                <li><b><a href="#falcon">Falcon-H1-0.5B-Instruct-GGUF</a></b></li>
                <li><b><a href="#qwen">Qwen3-4B-Instruct-2507-GGUF</a></b></li>
                <li><b><a href="#lfm">LFM2-2.6B-GGUF</a></b></li>
            </ul>
            
            <!-- Compatibility Note -->
            <p>All models are compatible with <b>llama.cpp</b>, <b>Unsloth</b>, and <b>T4 GPUs</b>.</p>
            <p>üöÄ <a href="https://colab.research.google.com/drive/1D4renZPZeqvDLcHl44fDCO4beWiJ3ata?usp=sharing" target="_blank"><strong>Run GGUF Inference in Google Colab without GPUs</strong></a></p>

            <!-- Models Container -->
            <div id="models-container"></div>

            <!-- Performance Overview -->
            <h2 id="performance">Model Comparison</h2>
            <div style="overflow: auto; max-height: 600px;">
                <table id="comparison-table">
                    <thead>
                        <tr><th>Model</th><th>Params</th><th>Format</th><th>Quant</th><th>Size (approx)</th></tr>
                    </thead>
                    <tbody>
                        <!-- Data loaded from CSV -->
                    </tbody>
                </table>
            </div>

            <!-- Fine-tuning Section -->
            <h2 id="finetuning">Fine-tuning with Unsloth</h2>
            <p>
                We specialize in creating efficient fine-tuning workflows using <strong>Unsloth</strong>, which enables rapid training on consumer-grade hardware like T4 GPUs. Our methods are optimized for memory efficiency without sacrificing performance, making it possible to fine-tune models on local machines or free cloud instances.
            </p>
            <p>
                Our primary fine-tuning guide is available as a Jupyter notebook. It covers the end-to-end process for adapting models to your specific needs. Explore our step-by-step guide to start fine-tuning your own models.
            </p>
            <p>‚û°Ô∏è <a href="https://colab.research.google.com/drive/12CX-1mVg21w89hgShlmE3kYZkdHP2Q1d?usp=sharing" target="_blank">View Fine-tuning Notebook on colab</a></p>

            <!-- About Section -->
            <div class="spacer">
                <h2 id="about">About SS Labs</h2>

                <p><b>SS Labs</b> focuses on converting open LLMs to efficient GGUF formats, 
                    fine-tuning for local inference, and building multilingual text generation models.
                </p>
                <blockquote>‚ÄúEfficiency meets accessibility ‚Äî one model at a time.‚Äù</blockquote>
            </div>
            
            <!-- References Section -->
            <h2 id="references">References</h2>
            <ul>
                <li><a href="https://docs.unsloth.ai/get-started/fine-tuning-llms-guide" target="_blank">Unsloth Doc</a></li>
                <li><a href="https://colab.research.google.com/drive/1D4renZPZeqvDLcHl44fDCO4beWiJ3ata?usp=sharing" target="_blank">Google Colab Inference Notebook</a></li>
                <li><a href="https://colab.research.google.com/drive/12CX-1mVg21w89hgShlmE3kYZkdHP2Q1d?usp=sharing" target="_blank">Fine-tuning Notebook</a></li>
            </ul>

            <!-- Footer -->
            <footer>
                <p>¬© 2024 SS Labs. All rights reserved.</p>
            </footer>
        </div>
    </div>

    <script src="../static/js/script.js"></script>
    <script src="../static/js/coming_soon.js"></script>
    <script src="../static/js/new.js"></script>
    <script src="../static/js/models.js"></script>
    <script src="../static/js/model_comparison.js"></script>
</body>
</html>
