{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-05T05:23:30.479705Z",
     "iopub.status.busy": "2025-12-05T05:23:30.478960Z",
     "iopub.status.idle": "2025-12-05T05:27:16.503125Z",
     "shell.execute_reply": "2025-12-05T05:27:16.501978Z",
     "shell.execute_reply.started": "2025-12-05T05:23:30.479669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth xformers \"trl<0.9.0\" peft accelerate bitsandbytes wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:29:20.750801Z",
     "iopub.status.busy": "2025-12-05T05:29:20.750529Z",
     "iopub.status.idle": "2025-12-05T05:29:20.755062Z",
     "shell.execute_reply": "2025-12-05T05:29:20.754235Z",
     "shell.execute_reply.started": "2025-12-05T05:29:20.750782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "max_seq_length = 548  # Supports long context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:29:25.102536Z",
     "iopub.status.busy": "2025-12-05T05:29:25.102230Z",
     "iopub.status.idle": "2025-12-05T05:29:32.793204Z",
     "shell.execute_reply": "2025-12-05T05:29:32.792372Z",
     "shell.execute_reply.started": "2025-12-05T05:29:25.102517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import wandb\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Remember to visit the URLs below to get your Hugging Face and Weights & Biases (W&B) API keys!\n",
    "# Hugging Face: https://huggingface.co/settings/tokens\n",
    "# W&B:          https://wandb.ai/authorize\n",
    "\n",
    "# Access the API key\n",
    "HF_API_Key = \" \"\n",
    "WANDB_API_KEY = \" \"\n",
    "\n",
    "# Log into Hugging Face\n",
    "login(token=HF_API_Key)\n",
    "\n",
    "# Log into Wandb\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "\n",
    "print(\"Login setup complete!\")\n",
    "\n",
    "# Get the current user info\n",
    "user_info = whoami()\n",
    "\n",
    "\n",
    "# Print normal text line by line\n",
    "for key, value in user_info.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:29:59.824495Z",
     "iopub.status.busy": "2025-12-05T05:29:59.823252Z",
     "iopub.status.idle": "2025-12-05T05:30:18.138146Z",
     "shell.execute_reply": "2025-12-05T05:30:18.137443Z",
     "shell.execute_reply.started": "2025-12-05T05:29:59.824468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",  # 4-bit quantized base (1-3B ideal for beginners) unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # Auto-detect float16/bfloat16\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # LoRA rank (1-10% params: ~10M trainable for 1B model)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],  # QLoRA targets\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Memory-efficient\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:30:26.824108Z",
     "iopub.status.busy": "2025-12-05T05:30:26.823359Z",
     "iopub.status.idle": "2025-12-05T05:30:29.399568Z",
     "shell.execute_reply": "2025-12-05T05:30:29.398560Z",
     "shell.execute_reply.started": "2025-12-05T05:30:26.824073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:30:51.722449Z",
     "iopub.status.busy": "2025-12-05T05:30:51.721493Z",
     "iopub.status.idle": "2025-12-05T05:32:45.568789Z",
     "shell.execute_reply": "2025-12-05T05:32:45.567883Z",
     "shell.execute_reply.started": "2025-12-05T05:30:51.722385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=30,  # Adjust for full train (e.g., 1000+)\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to = \"wandb\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:32:58.946576Z",
     "iopub.status.busy": "2025-12-05T05:32:58.946181Z",
     "iopub.status.idle": "2025-12-05T05:33:04.689262Z",
     "shell.execute_reply": "2025-12-05T05:33:04.688445Z",
     "shell.execute_reply.started": "2025-12-05T05:32:58.946549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Explain machine learning in simple terms.\n",
    "\n",
    "### Input:\n",
    "None\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=164, use_cache=True)\n",
    "print(tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:35:36.530973Z",
     "iopub.status.busy": "2025-12-05T05:35:36.530684Z",
     "iopub.status.idle": "2025-12-05T05:35:45.861207Z",
     "shell.execute_reply": "2025-12-05T05:35:45.860321Z",
     "shell.execute_reply.started": "2025-12-05T05:35:36.530951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save LoRA adapters (~10MB)\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n",
    "\n",
    "# Load later\n",
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T05:37:34.610583Z",
     "iopub.status.busy": "2025-12-05T05:37:34.609665Z",
     "iopub.status.idle": "2025-12-05T05:42:59.929689Z",
     "shell.execute_reply": "2025-12-05T05:42:59.928865Z",
     "shell.execute_reply.started": "2025-12-05T05:37:34.610554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"model.gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"q4_k_m\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model.gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"q4_k_m\", # Only Q8_0, BF16, F16 supported\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
