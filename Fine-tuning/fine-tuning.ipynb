{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKqAk4YQsNXv"
   },
   "source": [
    "# **Install Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4oOySfVmjhk9"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install unsloth wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBog7VJolYAp"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This command installs the **Unsloth** library — a toolkit for **efficient LLM fine-tuning** (especially for models like Gemma, LLaMA, or Mistral).\n",
    "\n",
    "It downloads and installs Unsloth so you can use it in your notebook for model loading, LoRA fine-tuning, and quantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KzSK4or1LKC"
   },
   "source": [
    "# **Login to Hugging Face and Weights & Biases (WandB)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EBkykzmu0_Y6"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "from huggingface_hub import login, whoami\n",
    "\n",
    "# Access the API key\n",
    "HF_API_Key = \" \"\n",
    "WANDB_API_KEY = \" \"\n",
    "\n",
    "\n",
    "login(token=HF_API_Key)    # Log into Hugging Face\n",
    "wandb.login(key=WANDB_API_KEY)  # Log into Wandb\n",
    "\n",
    "\n",
    "print(\"Login setup complete!\")\n",
    "\n",
    "# Get the current user info\n",
    "user_info = whoami()\n",
    "\n",
    "# Print normal text line by line\n",
    "for key, value in user_info.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qNsv34V1RVC"
   },
   "source": [
    "**Simple Explanation:**\n",
    "\n",
    "This code logs you into **Hugging Face** and **WandB** using your API keys.\n",
    "\n",
    "* `HF_API_Key` — your **Hugging Face API token**.\n",
    "* `WANDB_API_KEY` — your **Weights & Biases API key**.\n",
    "* `login(token=HF_API_Key)` — connects your notebook to Hugging Face Hub.\n",
    "* `wandb.login(key=WANDB_API_KEY)` — connects your notebook to WandB for experiment tracking.\n",
    "* `whoami()` — checks and displays your current Hugging Face account info.\n",
    "\n",
    "**In short:**\n",
    "It authenticates your session so you can upload models to Hugging Face and track training progress with WandB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07F1BEtAshls"
   },
   "source": [
    "## **Loading a Pretrained Model using Unsloth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYgfxa8Ujlf3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"tiiuae/Falcon-H1-1.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=256,\n",
    "    dtype=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivHbQAs3lmnI"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `FastLanguageModel` is imported from **Unsloth**.\n",
    "* The model (e.g., **LLaMA**, **Gemma**, or any other Hugging Face-compatible model**) is being loaded.\n",
    "* `load_in_4bit=True` enables **4-bit quantization**, which significantly reduces VRAM usage while maintaining good performance.\n",
    "* `max_seq_length=256` sets the maximum number of tokens the model can process in a single input sequence.\n",
    "* `dtype=None` allows Unsloth to automatically choose the best precision type (like `torch.bfloat16` or `torch.float16`) based on your GPU.\n",
    "\n",
    "**Note:** You can replace the model name (e.g., `\"tiiuae/Falcon-E-3B-Instruct\"`) with any supported model such as **\"meta-llama/Llama-3-8B-Instruct\"**, **\"google/gemma-2b\"**, or **\"mistralai/Mistral-7B-Instruct\"** — Unsloth supports them all for efficient fine-tuning and inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIpYAmVus-31"
   },
   "source": [
    "## **We now add LoRA adapters so we only need to update 1 to 10% of all parameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQgysLS1jnkl"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.1,\n",
    "    use_gradient_checkpointing = False,\n",
    "    random_state = 1407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juo3AZivl6Cv"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `FastLanguageModel.get_peft_model()` — wraps the base model with **LoRA (Low-Rank Adaptation)** layers for parameter-efficient fine-tuning.\n",
    "* `r = 8` — sets the **LoRA rank**, which determines how many additional trainable parameters are added. Common values include **8, 16, 32, 64, 128**.\n",
    "* `target_modules` — specifies which model layers will be adapted using LoRA. Typically includes projection layers such as `\"q_proj\"`, `\"v_proj\"`, `\"o_proj\"`, `\"gate_proj\"`, `\"up_proj\"`, and `\"down_proj\"`.\n",
    "* `lora_alpha = 8` — a scaling factor that adjusts the impact of LoRA updates on the base model.\n",
    "* `lora_dropout = 0.1` — applies dropout regularization within LoRA layers to reduce overfitting.\n",
    "* `use_gradient_checkpointing = False` — can be set to `True` to save GPU memory during training (slightly slower due to recomputation).\n",
    "* `random_state = 1407` — ensures reproducibility by setting a consistent random seed.\n",
    "\n",
    "**Note:**\n",
    "This configuration works not just for **Falcon**, but also for other transformer models like **LLaMA**, **Gemma**, **Mistral**, or **Yi**.\n",
    "You only need to adjust the `target_modules` list if your chosen model uses different layer naming conventions (e.g., `\"Wqkv\"` for Gemma or `\"attn.q_proj\"` for LLaMA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb7abF7WtdCE"
   },
   "source": [
    "# **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ama_A4bmjpbl"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFYKNaBymHV2"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `alpaca_prompt` — defines the **prompt template** used for instruction-tuning. It organizes the data into three sections: **Instruction**, **Input**, and **Response**, following the Alpaca dataset format.\n",
    "* `EOS_TOKEN` — marks the **end of a sentence (end-of-sequence)**, ensuring the model knows when to stop generating text.\n",
    "* `formatting_prompts_func()` — formats raw dataset examples into the Alpaca-style structure by inserting each instruction, input, and output into the template, and then appends the `EOS_TOKEN`.\n",
    "* `load_dataset(\"yahma/alpaca-cleaned\")` — loads a **cleaned and high-quality version** of the Alpaca dataset, containing structured instruction–response pairs for supervised fine-tuning.\n",
    "* `dataset.map(..., batched=True)` — efficiently applies the formatting function to all dataset entries in batches for faster preprocessing.\n",
    "\n",
    "**In essence:**\n",
    "This code **prepares and tokenizes the training data** into a clean, structured **instruction-following format**, ready for fine-tuning large language models such as **LLaMA**, **Gemma**, **Mistral**, or **any Hugging Face-compatible model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx50khC3t1Rs"
   },
   "source": [
    "# **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHH7A2DTjq7t"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=256,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"wandb\",  # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pODe8Er8miMe"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `from trl import SFTConfig, SFTTrainer` — imports classes from **TRL (Transformer Reinforcement Learning)**, designed for **Supervised Fine-Tuning (SFT)** of language models.\n",
    "* `trainer = SFTTrainer(...)` — initializes the **fine-tuning trainer**, which manages model training, dataset loading, and optimization automatically.\n",
    "\n",
    "---\n",
    "\n",
    "**Parameters Explained:**\n",
    "\n",
    "* `model` and `tokenizer` — refer to the preloaded **LLaMA**, **Gemma**, or **Falcon** models and their associated tokenizer.\n",
    "* `train_dataset=dataset` — specifies the formatted Alpaca dataset for training.\n",
    "* `dataset_text_field=\"text\"` — tells the trainer which dataset column contains the text input for fine-tuning.\n",
    "* `max_seq_length=256` — limits the maximum number of tokens per training example.\n",
    "* `dataset_num_proc=2` — enables multiprocessing with 2 CPU threads to speed up data preprocessing.\n",
    "* `packing=False` — keeps each sample separate (when `True`, multiple shorter samples can be packed together to improve efficiency).\n",
    "\n",
    "---\n",
    "\n",
    "**`SFTConfig` arguments:**\n",
    "\n",
    "* `per_device_train_batch_size=2` — sets batch size per GPU or device.\n",
    "* `gradient_accumulation_steps=8` — effectively increases the total batch size without exceeding GPU memory limits.\n",
    "* `warmup_steps=5` — gradually warms up the learning rate for more stable early training.\n",
    "* `num_train_epochs=1` — runs one complete pass through the dataset (can be increased for full training).\n",
    "* `max_steps=10` — limits training to 10 steps, useful for debugging or quick tests.\n",
    "* `learning_rate=2e-4` — the learning rate for the optimizer.\n",
    "* `optimizer=\"adamw_8bit\"` — uses **8-bit AdamW** from BitsAndBytes to save GPU memory.\n",
    "* `weight_decay=0.001` — adds a small penalty to reduce overfitting.\n",
    "* `lr_scheduler_type=\"linear\"` — linearly decays the learning rate over time.\n",
    "* `seed=3407` — sets a fixed random seed for reproducibility.\n",
    "* `output_dir=\"Outputs\"` — specifies where to save model checkpoints and logs.\n",
    "* `report_to=\"none\"` — disables logging to external tools like **WandB** or **TensorBoard** (can be changed to `\"wandb\"` for experiment tracking).\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "This code configures and initializes the **training process** for fine-tuning **LLaMA**, **Gemma**, or **Falcon** models using **Unsloth** and **TRL’s `SFTTrainer`**.\n",
    "It enables **efficient, low-memory supervised fine-tuning** on structured instruction datasets such as **Alpaca** or custom instruction-response data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnMix1BomuDV"
   },
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8av3JlZ0jsy-"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6s4s_KmsJ_"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `trainer_stats = trainer.train()` — begins the **fine-tuning process** using the previously defined configuration and dataset with `SFTTrainer`.\n",
    "\n",
    "---\n",
    "\n",
    "**Details:**\n",
    "\n",
    "* This command **starts the main training loop**, running forward and backward passes over the dataset.\n",
    "* Only the **LoRA (Low-Rank Adaptation)** layers are updated, while the **base model (e.g., LLaMA, Gemma, or Falcon)** remains mostly frozen to save memory and training time.\n",
    "* The `trainer` automatically manages:\n",
    "\n",
    "  * Loading and batching data from the dataset\n",
    "  * Calculating the loss function\n",
    "  * Performing backpropagation\n",
    "  * Accumulating gradients for efficient training\n",
    "  * Saving model checkpoints and logs (if enabled)\n",
    "\n",
    "---\n",
    "\n",
    "**Output — `trainer_stats`:**\n",
    "\n",
    "The `trainer_stats` object contains key training metrics:\n",
    "\n",
    "* **Training loss** per iteration or epoch\n",
    "* **Learning rate** evolution throughout training\n",
    "* **Total training duration**\n",
    "* **Number of completed steps or epochs**\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "This cell executes the **core training loop** for fine-tuning **LLaMA**, **Gemma**, or **any other supported model** using **LoRA adapters** and the **Alpaca instruction dataset** — achieving efficient, parameter-light supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSihbb7iuZat"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c8q95BDucid"
   },
   "source": [
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQAyIOucjumW"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyOHOp6hm319"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `FastLanguageModel.for_inference(model)` — switches the fine-tuned model into **inference mode**, disabling gradient calculations and enabling optimized settings for **faster text generation (up to 2× faster)**.\n",
    "* `inputs = tokenizer([...], return_tensors=\"pt\").to(\"cuda\")` — tokenizes the input text into **PyTorch tensors** and moves them to the GPU for efficient inference.\n",
    "\n",
    "---\n",
    "\n",
    "**Prompt Structure:**\n",
    "\n",
    "* The `alpaca_prompt.format()` template is reused to maintain the same structured input format:\n",
    "\n",
    "  * **Instruction:** `\"Continue the fibonacci sequence.\"`\n",
    "  * **Input:** `\"1, 1, 2, 3, 5, 8\"`\n",
    "  * **Output:** (left empty) — this signals the model to **generate** the continuation.\n",
    "\n",
    "---\n",
    "\n",
    "**Generation Step:**\n",
    "\n",
    "* `outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)`\n",
    "\n",
    "  * `max_new_tokens=64` — defines the maximum number of tokens the model can generate beyond the input prompt.\n",
    "  * `use_cache=True` — enables caching of past key values to accelerate token generation.\n",
    "\n",
    "---\n",
    "\n",
    "**Decoding:**\n",
    "\n",
    "* `tokenizer.batch_decode(outputs)` — converts the generated token IDs back into human-readable text.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**\n",
    "This code performs **text generation (inference)** using your fine-tuned **LLaMA**, **Gemma**, or any other compatible model.\n",
    "It demonstrates how to use the trained model to produce **coherent, context-aware outputs** from structured instruction–input prompts (like continuing a Fibonacci sequence or answering natural language questions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjUd6X4Luo8b"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "A2Cv4ZRQjxQf"
   },
   "outputs": [],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPQf1iAWnBR1"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `FastLanguageModel.for_inference(model)` — puts the model into **inference mode**, disabling training layers and optimizing it for **fast text generation** (around **2× faster performance**).\n",
    "\n",
    "* `inputs = tokenizer([...], return_tensors=\"pt\").to(\"cuda\")` — tokenizes the given instruction and input text, converting them into **PyTorch tensors** and moving them to the GPU for generation.\n",
    "\n",
    "  * Uses the `alpaca_prompt.format()` template with:\n",
    "\n",
    "    * **Instruction:** `\"Continue the fibonacci sequence.\"`\n",
    "    * **Input:** `\"1, 1, 2, 3, 5, 8\"`\n",
    "    * **Output:** left blank — letting the model generate the continuation automatically.\n",
    "\n",
    "* `from transformers import TextStreamer` — imports the **TextStreamer** class from the Hugging Face Transformers library.\n",
    "\n",
    "* `text_streamer = TextStreamer(tokenizer)` — sets up a **real-time text streamer** that prints each generated token live, just like ChatGPT’s typing effect.\n",
    "\n",
    "* `model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)` — generates up to **128 new tokens**, streaming them to the output in real time instead of waiting until the model finishes generating the entire response.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "This code enables **real-time text generation (streaming inference)** using your fine-tuned **LLaMA**, **Gemma**, or **Mistral** models. It prints the model’s output **live, token by token**, making it interactive and similar to ChatGPT’s response style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95ba3AUZu5_T"
   },
   "source": [
    "# **Saving, loading finetuned models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dk6_uoTtjzle"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqF4dl40nMJ1"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `model.save_pretrained(\"lora_model\")` — saves the **fine-tuned model** (including the **LoRA adapter weights**) to a local directory named `\"lora_model\"`.\n",
    "\n",
    "  * This folder will include essential files such as `adapter_model.bin`, `config.json`, and other metadata needed to reload the model.\n",
    "  * The comment `# Local saving` indicates that this step saves the model locally — you can later upload it to **Hugging Face Hub** or reuse it in **Colab** or your **local environment**.\n",
    "\n",
    "* `tokenizer.save_pretrained(\"lora_model\")` — saves the **tokenizer configuration**, including the vocabulary, merges, and special tokens, to the same `\"lora_model\"` directory.\n",
    "\n",
    "  * This ensures that the model uses **the same tokenizer setup** when you reload it, maintaining consistent text encoding and decoding behavior.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "This code **exports your fine-tuned model and tokenizer** (such as **Gemma**, **LLaMA**, or **Mistral**) with their **LoRA adapters** into a reusable folder.\n",
    "You can easily reload them later for inference or further fine-tuning using:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lora_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n",
    "```\n",
    "\n",
    "This allows you to run or share your **custom fine-tuned model** seamlessly across different environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qcn8JvLevSS0"
   },
   "source": [
    "### **Saving to float16 for VLLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeXcW9Ybj12u"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF51TE4Wocmt"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "* `device_map=\"auto\"` — automatically assigns model layers to available hardware (e.g., GPU, CPU, or multiple GPUs).\n",
    "\n",
    "  * This ensures optimal memory usage without manually specifying devices.\n",
    "\n",
    "* `model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")` —\n",
    "  saves a **merged version** of the fine-tuned model.\n",
    "\n",
    "What happens here:\n",
    "\n",
    "* During LoRA fine-tuning, only **adapter layers** (LoRA weights) are trained.\n",
    "* This function **merges** those LoRA adapters with the **base model weights**, creating a **single unified model**.\n",
    "* The model is saved in **16-bit precision (FP16)** — reducing file size while keeping high accuracy.\n",
    "\n",
    "Directory structure:\n",
    "\n",
    "After execution, a new folder named **`merged_model/`** is created containing:\n",
    "\n",
    "* The full merged model weights (ready for inference or deployment).\n",
    "* The tokenizer files for text encoding/decoding.\n",
    "* The configuration (`config.json`) describing model parameters.\n",
    "\n",
    "**In short:**\n",
    "This step produces a **final deployable model** (e.g., based on **Gemma**, **Llama**, or any other compatible model) by merging LoRA fine-tuned weights with the original base model into a single efficient checkpoint — ideal for sharing or inference without LoRA dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-2wk_hPp4PZ"
   },
   "source": [
    "# **GGUF & llama.cpp Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvGuDmjEj4PV"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# STEP 1: Clone llama.cpp repository\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "# STEP 2: Build llama.cpp (for quantization binary)\n",
    "%cd llama.cpp\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fallocate -l 8G /swapfile\n",
    "chmod 600 /swapfile\n",
    "mkswap /swapfile\n",
    "swapon /swapfile\n",
    "\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j2\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEWPGHxLooxD"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This code prepares **`llama.cpp`**, a high-performance C++ framework for **quantizing and running large language models (LLMs)** such as **LLaMA**, **Gemma**, or **Mistral** — enabling efficient local inference on **CPUs or low-VRAM GPUs**.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Clone llama.cpp repository**\n",
    "\n",
    "```bash\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "```\n",
    "\n",
    "* Downloads the official **llama.cpp** project from GitHub.\n",
    "* This repository provides utilities for:\n",
    "\n",
    "  * **Quantization** (reducing model size and memory usage)\n",
    "  * **Model conversion** from Hugging Face to GGUF format\n",
    "  * **Local inference** optimized in C/C++ (no need for PyTorch).\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Build llama.cpp (compile the binaries)**\n",
    "\n",
    "```bash\n",
    "%cd llama.cpp\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release -j\n",
    "%cd ..\n",
    "```\n",
    "\n",
    "* `%cd llama.cpp` — changes the working directory to the cloned repo.\n",
    "* `!cmake -B build` — generates build configuration files in a `build/` folder.\n",
    "* `!cmake --build build --config Release -j` — compiles the source code using multiple CPU threads (`-j` speeds up compilation).\n",
    "* `%cd ..` — returns to the root directory once the build finishes.\n",
    "\n",
    "---\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "After building, you’ll get executable binaries in `llama.cpp/build/bin`, including:\n",
    "\n",
    "* **`quantize`** — converts models (e.g., **Gemma**, **LLaMA**, or **Mistral**) into **GGUF format** for smaller, faster versions.\n",
    "* **`main`** — runs **inference locally** on CPUs or GPUs efficiently without requiring Python or PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "This step installs and compiles the **llama.cpp toolchain**, giving you the ability to **quantize and run your fine-tuned models locally** — perfect for deploying **Gemma**, **LLaMA**, or similar models on lightweight hardware with minimal dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saEmbId1pzwZ"
   },
   "source": [
    "## **Then, save the model to F16:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D8R-HPxj6T1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# For BF16:\n",
    "!python llama.cpp/convert_hf_to_gguf.py merged_model \\\n",
    "    --outfile model-BF16.gguf --outtype bf16 \\\n",
    "    --split-max-size 50G\n",
    "\n",
    "# For Q8_0:\n",
    "!python llama.cpp/convert_hf_to_gguf.py merged_model \\\n",
    "    --outfile gemma-2-2b-Q8_0.gguf --outtype q4_0 \\\n",
    "    --split-max-size 50G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMZd13Mto7mq"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "This code converts a **Hugging Face model** (for example, **Gemma**, **LLaMA**, or **Mistral**) into the **GGUF format** — the format used by `llama.cpp` for optimized inference.\n",
    "It supports both **full-precision** and **quantized** outputs for maximum efficiency across different hardware.\n",
    "\n",
    "The conversion is done using the script `convert_hf_to_gguf.py` provided in the **llama.cpp** repository.\n",
    "\n",
    "---\n",
    "\n",
    "**Step-by-Step Breakdown**\n",
    "\n",
    "**1️⃣ Convert to BF16 (Full Precision)**\n",
    "\n",
    "```bash\n",
    "!python llama.cpp/convert_hf_to_gguf.py merged_model \\\n",
    "    --outfile model-BF16.gguf --outtype bf16 \\\n",
    "    --split-max-size 50G\n",
    "```\n",
    "\n",
    "* **`merged_model`** — the directory containing your fine-tuned Hugging Face model (e.g., **Gemma**, **LLaMA**, or **Mistral**).\n",
    "* **`--outfile model-BF16.gguf`** — the name of the output GGUF file.\n",
    "* **`--outtype bf16`** — converts model weights to **bfloat16 precision**, preserving near-original quality.\n",
    "* **`--split-max-size 50G`** — automatically splits large models into 50 GB chunks to handle filesystem limits.\n",
    "\n",
    "**Use this mode** if you have enough VRAM or system memory and want **maximum accuracy** (ideal for high-end GPUs or research use).\n",
    "\n",
    "---\n",
    "\n",
    "**2️⃣ Convert to Q4_0 (Quantized Model)**\n",
    "\n",
    "```bash\n",
    "!python llama.cpp/convert_hf_to_gguf.py merged_model \\\n",
    "    --outfile gemma-2b-q4_0.gguf --outtype q4_0 \\\n",
    "    --split-max-size 50G\n",
    "```\n",
    "\n",
    "* **`--outtype q4_0`** — converts the model to **4-bit quantization**, reducing its size by up to **80%** while maintaining good performance.\n",
    "* **`--outfile gemma-2b-q4_0.gguf`** — sets the output file name for the quantized version.\n",
    "* Quantized models like `Q4_0`, `Q5_K`, or `Q8_0` are **optimized for inference** on CPUs or smaller GPUs.\n",
    "\n",
    "**Use this mode** for **lightweight and fast inference** in tools like **llama.cpp**, **Ollama**, or **LM Studio**, even on modest hardware.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Mode            | Output Type | Precision     | Use Case                                |\n",
    "| --------------- | ----------- | ------------- | --------------------------------------- |\n",
    "| **BF16**        | `bf16`      | 16-bit float  | High-quality, full-precision inference  |\n",
    "| **Q4_0 / Q8_0** | Quantized   | 4-bit / 8-bit | Lightweight, memory-efficient inference |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "This process converts your **Gemma**, **LLaMA**, or other Hugging Face-compatible model into a `.gguf` file — ready for **ultra-efficient local inference** using **llama.cpp**, **Ollama**, or similar runtimes.\n",
    "You can choose between **BF16 (full precision)** for accuracy or **Q4/Q8 (quantized)** for speed and lower memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRtuN-3Rpvib"
   },
   "source": [
    "### **Quantizing Model to Q3_K, q4_K_M & more Format using llama.cpp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2v6E3cVj_hV"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /kaggle/working/llama.cpp/build/bin\n",
    "./llama-quantize /kaggle/working/Phi-3-mini-4k-instruct-BF16.gguf \\\n",
    "                 /kaggle/working/Phi-3-mini-4k-instruct-q4_K.gguf \\\n",
    "                 q3_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlFquHPBpg8z"
   },
   "source": [
    "**Explanation**\n",
    "\n",
    "\n",
    "This Bash script performs **model quantization** using the **llama.cpp** tool, converting a full-precision **BF16 model** into a **Q3_K quantized model** — significantly reducing size and memory requirements while maintaining reasonable accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**Step-by-Step Breakdown**\n",
    "\n",
    "**1️⃣ Navigate to the llama.cpp build directory**\n",
    "\n",
    "```bash\n",
    "cd /kaggle/working/llama.cpp/build/bin\n",
    "```\n",
    "\n",
    "* Changes the working directory to where the **compiled llama.cpp binaries** are located.\n",
    "* This folder contains the executable tools, such as `llama-quantize`, used for model compression.\n",
    "\n",
    "---\n",
    "\n",
    "**2️⃣ Run the Quantization Command**\n",
    "\n",
    "```bash\n",
    "./llama-quantize /kaggle/working/Phi-3-mini-4k-instruct-BF16.gguf \\\n",
    "/kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf \\\n",
    "q3_K\n",
    "```\n",
    "\n",
    "* **`./llama-quantize`** — executes the quantization tool built from `llama.cpp`.\n",
    "* **Input file:**\n",
    "  `/kaggle/working/Phi-3-mini-4k-instruct-BF16.gguf` — this is your **BF16 (full-precision)** GGUF model file.\n",
    "* **Output file:**\n",
    "  `/kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf` — the resulting **Q3_K quantized** model file.\n",
    "* **Quantization type:**\n",
    "  `q3_K` — specifies the **3-bit quantization method**, providing a balance between speed, size, and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**About Q3_K Quantization**\n",
    "\n",
    "* **Q3_K** reduces the model weights to **3-bit precision**, drastically shrinking the file size (often 70–80% smaller).\n",
    "* It’s ideal for **CPU inference** or **low-memory GPU systems**.\n",
    "* The quantized model retains good performance while allowing fast and efficient inference.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "This script converts your **full-precision model (BF16)** into a **Q3_K quantized version**, ready for deployment using `llama.cpp`, **Ollama**, or **LM Studio**.\n",
    "It’s especially useful for running models like **LLaMA**, **Gemma**, or **Mistral** on limited hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDbC3-W4p9wZ"
   },
   "source": [
    "# **Push to Huggingface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bant_iOgkJA1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hf upload ss-lab/Phi-3-mini-4k-instruct-GGUF \\\n",
    "    /kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xrOpvRVqmNw"
   },
   "source": [
    "\n",
    "**Explanation**\n",
    "\n",
    "This command uploads your **quantized GGUF model** (for example, a **Gemma** or **LLaMA** model) to the **Hugging Face Hub**, making it available for public or private use.\n",
    "\n",
    "---\n",
    "\n",
    "**Breakdown**\n",
    "\n",
    "```bash\n",
    "!hf upload ss-lab/Phi-3-mini-4k-instruct-GGUF \\\n",
    "    /kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf\n",
    "```\n",
    "\n",
    "**1️⃣ `!hf upload`**\n",
    "\n",
    "* Uses the **Hugging Face CLI** to upload a file or model to your Hugging Face account or organization repository.\n",
    "* You can upload model weights, tokenizers, or even full project folders.\n",
    "\n",
    "---\n",
    "\n",
    "**2️⃣ `ss-lab/Phi-3-mini-4k-instruct-GGUF`**\n",
    "\n",
    "* This is the **destination repository** on Hugging Face.\n",
    "* It follows the format:\n",
    "\n",
    "  ```\n",
    "  username-or-org/repository-name\n",
    "  ```\n",
    "* Example alternatives:\n",
    "\n",
    "  * `your-username/Gemma-2B-GGUF`\n",
    "  * `your-username/Llama-3-8B-GGUF`\n",
    "* The repository will be created automatically if it doesn’t exist.\n",
    "\n",
    "---\n",
    "\n",
    "**3️⃣ `/kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf`**\n",
    "\n",
    "* This is the **local path** of the GGUF file you want to upload.\n",
    "* Change it depending on your environment:\n",
    "\n",
    "  * **Kaggle:** `/kaggle/working/your-model.gguf`\n",
    "  * **Colab:** `/content/your-model.gguf`\n",
    "  * **Local System:** `~/models/your-model.gguf`\n",
    "\n",
    "---\n",
    "\n",
    "**What it Does**\n",
    "\n",
    "* Uploads the `.gguf` model file (quantized or full-precision) to the Hugging Face Hub.\n",
    "* Once uploaded, the model becomes accessible via:\n",
    "\n",
    "  ```\n",
    "  https://huggingface.co/ss-lab/Phi-3-mini-4k-instruct-GGUF\n",
    "  ```\n",
    "* You (or others) can then download it with:\n",
    "\n",
    "  ```python\n",
    "  from huggingface_hub import hf_hub_download\n",
    "  hf_hub_download(repo_id=\"ss-lab/Phi-3-mini-4k-instruct-GGUF\", filename=\"Phi-3-mini-4k-instruct-q3_K.gguf\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "This command **pushes your locally quantized model (.gguf)** to the **Hugging Face Hub** for storage, sharing, or deployment — supporting any model like **Gemma**, **LLaMA**, or **Mistral**, not just Phi.\n",
    "It’s the final step to make your fine-tuned or quantized model available for easy download and inference anywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmGS0c7Qq19I"
   },
   "source": [
    "# **Removes GGUF From Local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_At0zyKzkKyU"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm /kaggle/working/Phi-3-mini-4k-instruct-q3_K.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRwUMznWrbHR"
   },
   "source": [
    "**Explanation:**\n",
    "\n",
    "\n",
    "It permanently deletes `model-q3_K.gguf` from the `/kaggle/working/` folder.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfEoqub5PodcHRBmIK76FO",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
