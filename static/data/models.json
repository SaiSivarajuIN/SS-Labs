[
    {
        "id": "falcon",
        "name": "Falcon-H1-0.5B-Instruct-GGUF",
        "repo": "Falcon-H1-0.5B-Instruct-GGUF",
        "details": {
            "Model Size": "0.5B parameters",
            "Architecture": "Falcon H1",
            "Quantization": "Q8_0 (556 MB)",
            "License": "falcon-llm-license"
        },
        "files": [
            "Falcon-H1-0.5B-Instruct-Q8_0.gguf"
        ]
    },
    {
        "id": "gemma-3",
        "name": "Google-gemma-3-1b-it-GGUF",
        "repo": "Google-gemma-3-1b-it-GGUF",
        "details": {
            "Model Size": "1B parameters",
            "Architecture": "Gemma 3 1B IT",
            "Quantization": "Q4_0 (720 MB) / Q8_0 (1.07 GB) / BF16 (2.01 GB)",
            "License": "Gemma Terms of Use"
        },
        "files": [
            "Google-gemma-3-1b-it-Q4_0.gguf",
            "Google-gemma-3-1b-it-Q8_0.gguf",
            "Google-gemma-3-1b-it-BF16.gguf"
        ]
    },
    {
        "id": "ibm-granite",
        "name": "IBM-granite-4.0-micro-GGUF",
        "repo": "IBM-granite-4.0-micro-GGUF",
        "details": {
            "Model Size": "3B parameters",
            "Architecture": "IBM Granite 4.0 Micro",
            "Quantization": "Q4_0 (1.98 GB) / Q8_0 (3.62 GB)",
            "License": "apache 2.0"
        },
        "files": [
            "IBM-granite-4.0-micro-Q4_0.gguf",
            "IBM-granite-4.0-micro-Q8_0.gguf"
        ]
    },
    {
        "id": "lfm",
        "name": "LFM2-2.6B-GGUF",
        "repo": "LFM2-2.6B-GGUF",
        "details": {
            "Model Size": "2.6B parameters",
            "Architecture": "LFM 2",
            "Quantization": "Q4_0 (1.48 GB)",
            "License": "lfm1.0"
        },
        "files": [
            "LFM2-2.6B-Q4_0.gguf"
        ]
    },
    {
        "id": "lfm-exp",
        "name": "LFM2-2.6B-Exp-GGUF",
        "repo": "LFM2-2.6B-Exp-GGUF",
        "details": {
            "Model Size": "2.6B parameters",
            "Architecture": "LFM 2 Exp",
            "Quantization": "Q2_K (1.56 GB) / Q8_0 (2.73 GB) / BF16 (5.14 GB)",
            "License": "lfm1.0"
        },
        "files": [
            "LiquidAI-LFM2-2.6B-Exp-Q2_K.gguf",
            "LiquidAI-LFM2-2.6B-Exp-Q8_0.gguf",
            "LiquidAI-LFM2-2.6B-Exp-BF16.gguf"
        ]
    },
    {
        "id": "llama3",
        "name": "Llama-3-8b-bnb-4bit-GGUF",
        "repo": "Llama-3-8b-bnb-4bit-GGUF",
        "details": {
            "Model Size": "8B parameters",
            "Architecture": "Llama 3",
            "Quantization": "Q4_K_M (~5.0 GB)",
            "License": "llama3"
        },
        "files": [
            "llama-3-8b.Q4_K_M.gguf"
        ]
    },
    {
        "id": "llama3.2",
        "name": "Llama-3.2-1B-Instruct-GGUF",
        "repo": "Llama-3.2-1B-Instruct-GGUF",
        "details": {
            "Model Size": "1B parameters",
            "Architecture": "Llama 3.2",
            "Quantization": "Q2_K (581 MB) / Q8_0 (1.32 GB) / BF16 (2.48 GB)",
            "License": "llama3"
        },
        "files": [
            "Llama-3.2-1B-Instruct-Q2_K.gguf",
            "Llama-3.2-1B-Instruct-Q8_0.gguf",
            "Llama-3.2-1B-Instruct-BF16.gguf"
        ]
    },
    {
        "id": "ministral-8b",
        "name": "Ministral-8B-Instruct-2410-GGUF",
        "repo": "Ministral-8B-Instruct-2410-GGUF",
        "details": {
            "Model Size": "8B parameters",
            "Architecture": "Ministral 8B Instruct 2410",
            "Quantization": "Q4_0 (4.66 GB) / Q8_0 (8.53 GB)",
            "License": "apache-2.0"
        },
        "files": [
            "Ministral-8B-Instruct-2410-Q4_0.gguf",
            "Ministral-8B-Instruct-2410-Q8_0.gguf"
        ]
    },
    {
        "id": "mistral",
        "name": "Mistral-7b-v0.3-bnb-4bit-GGUF",
        "repo": "Mistral-7b-v0.3-bnb-4bit-GGUF",
        "details": {
            "Model Size": "7B parameters",
            "Architecture": "Mistral 7B v0.3",
            "Quantization": "Q3_K_M (3.52 GB) / Q4_K_M (4.37 GB)",
            "License": "apache-2.0"
        },
        "files": [
            "mistral-7b-v0.3.Q3_K_M.gguf",
            "mistral-7b-v0.3.Q4_K_M.gguf"
        ]
    },
    {
        "id": "mistral-nemo",
        "name": "Mistral-Nemo-Instruct-2407-GGUF",
        "repo": "Mistral-Nemo-Instruct-2407-bnb-GGUF",
        "details": {
            "Model Size": "12B parameters",
            "Architecture": "Mistral Nemo Instruct 2407",
            "Quantization": "TQ1_0 (4.99 GB)",
            "License": "apache-2.0"
        },
        "files": [
            "Mistral-Nemo-Instruct-2407-bnb-tq1_0.gguf"
        ]
    },
    {
        "id": "phi",
        "name": "Phi-3.5-mini-instruct-GGUF",
        "repo": "Phi-3.5-mini-instruct-GGUF",
        "details": {
            "Model Size": "3.8B parameters",
            "Architecture": "Phi-3.5 Mini Instruct",
            "Quantization": "Q4_K_M (2.2 GB)",
            "License": "MIT"
        },
        "files": [
            "phi-3.5-mini-instruct-Q4_K_M.gguf"
        ]
    },
    {
        "id": "phi-4k",
        "name": "Phi-3-mini-4k-instruct-GGUF",
        "repo": "Phi-3-mini-4k-instruct-GGUF",
        "details": {
            "Model Size": "3.8B parameters",
            "Architecture": "Phi-3 Mini 4k Instruct",
            "Quantization": "Q4_K_M (2.2 GB)",
            "License": "MIT"
        },
        "files": [
            "Phi-3-mini-4k-instruct-q4_K_M.gguf"
        ]
    },
    {
        "id": "qwen",
        "name": "Qwen3-4B-Instruct-2507-GGUF",
        "repo": "Qwen3-4B-Instruct-2507-GGUF",
        "details": {
            "Model Size": "4B parameters",
            "Architecture": "Qwen 3",
            "Quantization": "Q2_K (1.67 GB) / Q4_0 (2.37 GB) / Q8_0 (4.28 GB)",
            "License": "apache-2.0"
        },
        "files": [
            "Qwen3-4B-Instruct-2507-Q2_K.gguf",
            "Qwen3-4B-Instruct-2507-Q4_0.gguf",
            "Qwen3-4B-Instruct-2507-Q8_0.gguf"
        ]
    },
    {
        "id": "qwen-thinking",
        "name": "Qwen3-4B-Thinking-2507-GGUF",
        "repo": "Qwen3-4B-Thinking-2507-GGUF",
        "details": {
            "Model Size": "4B parameters",
            "Architecture": "Qwen 3 Thinking",
            "Quantization": "Q2_K (1.67 GB) / Q4_K_M (2.5 GB) / Q8_0 (4.28 GB)",
            "License": "apache-2.0"
        },
        "files": [
            "Qwen3-4B-Thinking-2507-Q2_K.gguf",
            "Qwen3-4B-Thinking-2507-Q4_K_M.gguf",
            "Qwen3-4B-Thinking-2507-Q8_0.gguf"
        ]
    }
]